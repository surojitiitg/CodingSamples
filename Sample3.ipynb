{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code to install the required packages to operate PyG\n",
        "\n"
      ],
      "metadata": {
        "id": "Rm5Ckjm5TTiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E19iLRMTTCuN",
        "outputId": "6464a9b4-2086-441b-9c2e-cac407b7d893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper function for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize(h, color):\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to Split up an MNIST image into 4 patches"
      ],
      "metadata": {
        "id": "LZp1WPlpTSbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "class Patch(object):\n",
        "    \"\"\"\n",
        "    Creates patches from images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patch_size=14):\n",
        "      self.patch_size = patch_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "      #print(\"hello\")\n",
        "      #print(image)\n",
        "      #print(image.shape)\n",
        "      patched_image = image.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n",
        "      # patched_image = rearrange(image.unsqueeze(0), 'b c (h h1) (w w1) -> (b h1 w1) c h w', h1=2,w1=2)\n",
        "      # patched_image = rearrange(F.unfold(image, patch_size, patch_size), '(h w) c -> c h w', h=patch_size)\n",
        "      v = torch.flatten(patched_image)\n",
        "      #print(v)\n",
        "      patched_image = torch.reshape(v,(4,14,14))\n",
        "      #print(patched_image.shape)\n",
        "      #print(patched_image)\n",
        "\n",
        "      return patched_image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     Patch(),\n",
        " ])\n",
        "\n",
        "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(mnist_data))\n",
        "test_size = len(mnist_data) - train_size\n",
        "train_dataset, test_dataset = random_split(mnist_data, [train_size, test_size])\n",
        "\n",
        "# data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
        "#                                           batch_size=64,\n",
        "#                                           shuffle=True)\n",
        "\n",
        "data_loader_train = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True)\n",
        "\n",
        "# train_size = int(0.8 * len(data_loader.dataset))\n",
        "# valid_size = len(data_loader) - train_size\n",
        "\n",
        "# train_dataset, valid_dataset = random_split(data_loader.dataset, [train_size, valid_size])\n",
        "# print(train_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cft8tHq7TSGz",
        "outputId": "5c06016c-269e-4eaf-80ba-808b911965b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 131763432.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 21631373.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 45035596.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4768592.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code below is used to generate Dataloader (Train and Test) to be used with PyG.\n",
        "For the label, the first device has the accurate label of the MNIST digit and the other devices are assigned zero (primarily because after the first message passing round, the other devices will have zero, in the simple VFL case). The edge matrix defined below is for a VFL case, where the first device (out of 4) acts as both the client and server."
      ],
      "metadata": {
        "id": "16H_MpOBT5zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edge_index = torch.tensor([[0, 0],\n",
        "                           [1, 0],\n",
        "                           [2, 0],\n",
        "                           [3, 0]], dtype=torch.long)"
      ],
      "metadata": {
        "id": "dCOkAfUET0UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "dataSet_VFL_FC = []\n",
        "for (img,label) in data_loader_train:\n",
        "  for i,y in enumerate(img):\n",
        "    sens1 = ((y[0].reshape(1,-1))[0,:]).tolist()\n",
        "    sens2 = ((y[1].reshape(1,-1))[0,:]).tolist()\n",
        "    sens3 = ((y[2].reshape(1,-1))[0,:]).tolist()\n",
        "    sens4 = ((y[3].reshape(1,-1))[0,:]).tolist()\n",
        "    x1 = torch.tensor([sens1,sens2,sens3,sens4],dtype = torch.float)\n",
        "    #y = [label[i].tolist() for x in range(4)]\n",
        "    y = [label[i].tolist(),0,0,0]\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "    data_obj = Data(x=x1, edge_index=edge_index.t(), y=y)\n",
        "    dataSet_VFL_FC.append(data_obj)\n",
        "\n",
        "train_loader_VFL_FC  = DataLoader(dataSet_VFL_FC, batch_size=64)\n",
        "\n",
        "\n",
        "dataSet_VFL_FC_test = []\n",
        "for (img,label) in data_loader_test:\n",
        "  for i,y in enumerate(img):\n",
        "    sens1 = ((y[0].reshape(1,-1))[0,:]).tolist()\n",
        "    sens2 = ((y[1].reshape(1,-1))[0,:]).tolist()\n",
        "    sens3 = ((y[2].reshape(1,-1))[0,:]).tolist()\n",
        "    sens4 = ((y[3].reshape(1,-1))[0,:]).tolist()\n",
        "    x1 = torch.tensor([sens1,sens2,sens3,sens4],dtype = torch.float)\n",
        "    #y = [label[i].tolist() for x in range(4)]\n",
        "    y = [label[i].tolist(),0,0,0]\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "    data_obj = Data(x=x1, edge_index=edge_index.t(), y=y)\n",
        "    dataSet_VFL_FC_test.append(data_obj)\n",
        "test_loader_VFL_FC = DataLoader(dataSet_VFL_FC_test, batch_size=64)"
      ],
      "metadata": {
        "id": "YHa6T0DSUr7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_loader_VFL_FC.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGxYyRdKdq9_",
        "outputId": "cdd8970b-5caf-4fab-d7e5-5ea11a54eb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Code below is a Custom aggregation function that is used for aggregating by concateniating features across multiple nodes"
      ],
      "metadata": {
        "id": "NRvGLwEzVwMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from torch import Tensor\n",
        "\n",
        "from torch_geometric.nn.aggr import Aggregation\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear, Parameter\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import aggr\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SimpleConv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class CustomMLP(Aggregation):\n",
        "    r\"\"\"Performs MLP aggregation in which the elements to aggregate are\n",
        "    flattened into a single vectorial representation, and are then processed by\n",
        "    a Multi-Layer Perceptron (MLP), as described in the `\"Graph Neural Networks\n",
        "    with Adaptive Readouts\" <https://arxiv.org/abs/2211.04952>`_ paper.\n",
        "\n",
        "    .. note::\n",
        "\n",
        "        :class:`GRUAggregation` requires sorted indices :obj:`index` as input.\n",
        "        Specifically, if you use this aggregation as part of\n",
        "        :class:`~torch_geometric.nn.conv.MessagePassing`, ensure that\n",
        "        :obj:`edge_index` is sorted by destination nodes, either by manually\n",
        "        sorting edge indices via :meth:`~torch_geometric.utils.sort_edge_index`\n",
        "        or by calling :meth:`torch_geometric.data.Data.sort`.\n",
        "\n",
        "    .. warning::\n",
        "\n",
        "        :class:`MLPAggregation` is not a permutation-invariant operator.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        max_num_elements (int): The maximum number of elements to aggregate per\n",
        "            group.\n",
        "        **kwargs (optional): Additional arguments of\n",
        "            :class:`torch_geometric.nn.models.MLP`.\n",
        "    \"\"\"\n",
        "    # def __init__(self, in_channels: int, out_channels: int,\n",
        "    #              max_num_elements: int, **kwargs):\n",
        "    def __init__(self, max_num_elements: int, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.in_channels = in_channels\n",
        "        # self.out_channels = out_channels\n",
        "        self.max_num_elements = max_num_elements\n",
        "\n",
        "        # from torch_geometric.nn import MLP\n",
        "        #self.mlp = MLP(in_channels=in_channels * max_num_elements,\n",
        "                       #out_channels=out_channels, **kwargs)\n",
        "\n",
        "        #self.reset_parameters()\n",
        "\n",
        "\n",
        "    # def reset_parameters(self):\n",
        "    #     self.mlp.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
        "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
        "                dim: int = -2) -> Tensor:\n",
        "        x, _ = self.to_dense_batch(x, index, ptr, dim_size, dim,\n",
        "                                   max_num_elements=self.max_num_elements)\n",
        "        return x.view(-1, x.size(1) * x.size(2))\n",
        "\n"
      ],
      "metadata": {
        "id": "BKwTfBcyV--j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used for defining the network"
      ],
      "metadata": {
        "id": "HNJ8fcZqWN_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import SimpleConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.nn import aggr\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SimpleConv(aggr = CustomMLP(max_num_elements=4))\n",
        "\n",
        "        self.lin = Linear(784, 120)\n",
        "        self.lin1 = Linear(120, 50)\n",
        "        self.lin2 = Linear(50, 10)\n",
        "\n",
        "    #def forward(self, x, edge_index): #Good when not using batching\n",
        "    def forward(self, data):\n",
        "      x, edge_index = data.x, data.edge_index\n",
        "      x = self.conv1(x, edge_index)\n",
        "      x = self.lin(x)\n",
        "      x = x.relu()\n",
        "\n",
        "      x = self.lin1(x)\n",
        "      x = x.relu()\n",
        "      x = self.lin2(x)\n",
        "\n",
        "\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = GCN(196,196)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "def test():\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  for batch in test_loader_VFL_FC:\n",
        "    output = model(batch)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    for batch_count in range(0,predicted.shape[0],4):\n",
        "      correct += int((predicted[batch_count]==batch.y[batch_count]).item())\n",
        "  return (correct) / (len(test_loader_VFL_FC.dataset))\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for batch in train_loader_VFL_FC:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(batch)\n",
        "    loss = criterion(output,batch.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss+= loss.item()\n",
        "  train_acc = test()\n",
        "  avg_loss = total_loss/len(train_loader_VFL_FC)\n",
        "  print(f'Epoch = {epoch}, train_loss = {avg_loss}, train_acc = {train_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2Xdum-5WV5T",
        "outputId": "a28e9b6a-5905-43bf-ef52-6273a4a61a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch = 0, train_loss = 0.2697514545197288, train_acc = 0.929\n",
            "Epoch = 1, train_loss = 0.04819771861284971, train_acc = 0.94875\n",
            "Epoch = 2, train_loss = 0.03316916544611255, train_acc = 0.9568333333333333\n",
            "Epoch = 3, train_loss = 0.024610665819762897, train_acc = 0.9629166666666666\n",
            "Epoch = 4, train_loss = 0.018891991714326043, train_acc = 0.9655\n",
            "Epoch = 5, train_loss = 0.014828129508843024, train_acc = 0.9675\n",
            "Epoch = 6, train_loss = 0.011844913529775417, train_acc = 0.966\n",
            "Epoch = 7, train_loss = 0.009372338285727892, train_acc = 0.9670833333333333\n",
            "Epoch = 8, train_loss = 0.007575706105097197, train_acc = 0.9679166666666666\n",
            "Epoch = 9, train_loss = 0.0062910464338977665, train_acc = 0.9663333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hOeLVhGei1Wa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}